---
title: "Claudeの自信作をKimiが4件潰した ── AIピアレビュー実践記"
emoji: "🔄"
type: "idea"
topics: ["claudecode", "ai", "kimi", "llm"]
published: true
---

Claude（Opus 4.6）にコンテンツ戦略を立てさせた。リタイトル6本、新テーマ5本。データに裏付けられた、隙のない提案だった。それを Kimi K2.5 に渡したら、6本中4件に指摘が入り、うち1件は「逆効果だからやめろ」と却下された。

1つのAIの提案を別のAIに批評させると、検討範囲が広がる。Claude 単体では出てこなかった視点が出現し、自分が無意識に同意していたポイントが可視化された。

## 背景——なぜバズ分析をしたか

21本の記事ポートフォリオ（公開18本＋下書き3本）を分析した。公開記事のタイトルを俯瞰してみると、「事実描写型」に偏重していることが分かった。18本中11本が「〜した話」「〜した記録」というパターンで、公開記事の61%を占める。

一方で、Zenn の週間トレンドを見ると「網羅型」「チェックリスト型」が上位に多い。自分のポートフォリオにはこの2パターンが1本もないというギャップがあった。意識的な選択ではなく、無意識のうちに「〜した話」という書き方に偏っていたことに気づいた。

## Claude の分析プロセス

Zenn API と Web検索でトレンドデータを収集し、歴代トレンド10本と週間トレンドから、バズるタイトルを9パターンに分類した。

| # | パターン | 例 |
|---|---|---|
| 1 | 挑発/断定型 | 「〜の真価は〇〇ではない」 |
| 2 | 網羅型 | 「〇〇完全ガイド」「〇〇選」 |
| 3 | チェックリスト型 | 「〇〇する前に確認すべきこと」 |
| 4 | 数値型 | 「9倍遅くなった」「0行で」 |
| 5 | 仮定/結果型 | 「Xしたら Yになった」 |
| 6 | 内幕公開型 | 「〇〇の裏側」「全貌」 |
| 7 | フロー追跡型 | 「〇〇を1ヶ月続けた記録」 |
| 8 | OSS公開型 | 「〇〇を作って公開した」 |
| 9 | 暗黙知の言語化型 | 「シニアエンジニアが無意識にやっていること」 |

この分類を自分の記事と対比し、ギャップを特定した。次にリタイトル提案6本とタイトル設計ルールを策定した。この時点で Claude の出力は論理的に整合しており、データの裏付けもあった。自分もこの提案に違和感を覚えなかった。

## Kimi に渡した理由と方法

Claude 単体には構造的な限界がある。自分の分析を裏付けるデータばかり重視し、「この仮説を崩す視点」を自動的には探さない。そして厄介なのは、その提案を読む自分もまた、データが揃っていると「もっともらしい」と感じてしまうことだった。

そこで Kimi K2.5 を使った。Claude とは異なるモデルアーキテクチャ（MoE: Mixture of Experts, 1兆パラメータ）で、学習データの構成も異なるため、異なる視点を期待できた。既に手元の開発環境に CLI ツールとして導入済みだったことも選定理由である（環境構築の詳細は[前回の記事](https://zenn.dev/shimo4228/articles/claude-kimi-hybrid-setup)で紹介している）。

ただし今回は実装委任ではなく、ピアレビューという異なるユースケースだった。プロンプトの構造は以下の通りだ。

```text
入力1: 著者の既存記事7本（Aランク）の全文
入力2: Claude の分析結果と提案の全文
指示: ストラテジスト・編集者・読者代表・マーケティングの4観点でレビューせよ
```

Kimi K2.5 は Agent Swarm アーキテクチャを持ち、内部オーケストレーターがタスクを分解して最大100体のサブエージェントに振り分ける。4観点を指定したのは、Claude の提案に対して異なる立場からの批評を明示的に求めるためだった。出力は約350行（17KB）。4つの観点それぞれに対して、具体的な指摘と代替案が返ってきた。

## Kimi が返したもの

6本のリタイトル提案のうち、2本はそのまま承認された（「Claude Code の真価はコード生成ではない」のサブタイトル削除、「Obsidian地獄」のタイトル変更）。残り4件について Kimi は以下のように判定した。なお、タイトル提案に加えて戦略面の補正も含まれている。

| Claude の提案 | Kimi の判定 | Kimi の理由（要約） |
|---|---|---|
| 「最強モデルで司令塔を組んだら9倍遅くなった」 | ⚠️ 修正 | 「棄却」の学びが消えている。本文の価値は「棄却の判断基準」にある |
| 「Claude Codeに397問の試験問題を自作し始めた」 | ❌ 却下 | 数字が主役になりすぎ。本文の核心「AIが自分の能力を提案しない」が霞む |
| 「文字数で最適ゾーンを狙う」戦略 | ⚠️ 補正 | 文字数より「情報密度」が正しい指標 |
| 「Claude Code で技術記事を20本書いて育てた Zenn 執筆環境の全貌」 | ⚠️ 要検討 | 「非エンジニア」押しを弱め実績数値で語るべき |

「397問」の却下が最も印象に残った。Claude の提案を読んだとき、「397問」という数字のインパクトに自分も引きずられていた。Kimi に「数字が主役になりすぎ」と指摘されて初めて気づいた。本文の核心は「Claude Code 自身が LLM なのに、その能力を提案しなかった」というメタ的洞察にある。数字の強さに目を奪われて、記事が本当に伝えたかったことを見失いかけていた。

「9倍遅くなった」についても同様の構造があった。数字だけに目が行き、「なぜ棄却したか」という判断プロセスの価値がタイトルから消えてしまう。Kimi の修正案は「なぜマルチエージェントを棄却したか」をサブタイトルに残す構成だった。

## 統合して分かったこと

**見えなかった同意が見えた。** これが最大の収穫だった。Claude の提案を読んだとき、データの裏付けがあったから「もっともらしい」と感じた。しかし Kimi の批評を読んで、自分が Claude の提案に無意識に同意していたことに気づいた。ピアレビューの価値は「正しい答えを出す」ことではなく、自分のバイアスを可視化することにあった。

**数字最適化の罠。** 「397問」は数字として強い。「9倍遅い」も衝撃がある。しかしその数字を前面に出すと、本文の学び——AIの盲点、棄却の判断基準——が犠牲になる。Kimi はこれを「逆効果」と呼んだ。コンテンツマーケティングの一般論ではあるが、自分の記事に当てはめたとき、具体的にどの数字がどの学びを消しているかを指摘されたのは、この体験でしか得られなかった解像度だった。

**同じツール、異なる価値。** Kimi K2.5 は[前回の記事](https://zenn.dev/shimo4228/articles/claude-kimi-hybrid-setup)ではコードを書くワーカーとして使った。今回は提案を批評するレビュアーとして使った。実装委任では Kimi の群知能（並列実行力）が活きるが、ピアレビューでは群知能の多角的視点が活きる。同じモデルでも、spec.md を渡すのと記事全文を渡すのとでは、引き出せる価値が全く異なった。

**この手法の限界。** ただし、Kimi の批評が「正しい」とは限らない。Kimi にも独自のバイアスがある。2つの AI が一致した場合にそれが正解とは限らないし、最終判断は人間が行っている以上、人間のバイアスも残る。ピアレビューで広がるのは「検討範囲」であって、「正解率」ではない。

## 実際にやった変更

以下を実行した。

- 当初6本のリタイトル提案のうち、Kimi の修正を反映して5本を確定
- zenn-writer スキルにタイトル設計7ルールを追加（「数字を前置し感情語と組み合わせる」「ただし学びの要素は残す」等）
- 新記事テーマ5本をリスト化（網羅型の「設定10選」、チェックリスト型の「LLM 出力を信じる前に」等、ギャップを埋めるテーマ）
- ブランディング移行の方向性を決定（「非エンジニアでもできた」→「Claude Code の限界を探る開拓者」）

これらの変更の効果はまだ検証できていない。リタイトル後の PV・いいね数の推移は今後追跡し、数値が揃った時点で報告する予定だ。

## まとめ

「AIを道具として使う」から「AIを議論相手として使う」への視点の転換だった。

```text
Claude（データ分析・構造化）
  → 著者（確認・承認）
    → Kimi（多角的批評）
      → 著者（統合・最終判断）
        → 実行
```

Claude はデータ分析と構造化を、Kimi は多角的批評とブランド整合性チェックを担った。この役割分担は、今回のピアレビューを通じて見えてきたものだ。

なお、この記事自体がバズ分析の成果を踏まえて設計されている。タイトルには「数値型」と「仮定/結果型」の複合、構成には「失敗→教訓型」と「具体→抽象→具体」の組み合わせを意図的に使った。この構成が実際に機能するかどうかは、この記事自身の PV・いいね数で検証される。

## 付録: この記事の制作プロセス

この記事自体が「AIピアレビュー」のプロセスで作られた。透明性のためにその過程を記録しておく。

1. **Claude が plan を作成。** 記事構造（8セクション）、タイトル候補3案、使用素材を設計した
2. **plan を spec.md に変換し、Kimi K2.5 にディスパッチ。** 初稿執筆を委任した。文体指定（だ/である調）、素材ファイルの参照パス、セクション構造を spec に記載。Kimi は素材3ファイルを自力で読み込み、約3,800文字の初稿を生成した。ただし初稿の品質は低かった。Claude と同じスキル群や MCP ツールへアクセスできる環境だったが、文章の肉付けや著者の声を反映する力では Claude に大きく劣る。構造は spec 通りだが、内容が薄く味気ない初稿だった
3. **Claude の editor エージェントが辛口レビュー。** 判定は「REVISE AND RESUBMIT」。数値の不整合3件（CRITICAL）、テーゼ検証の浅さや教訓の身体性不足など6件（MEDIUM）を指摘された
4. **Claude が editor フィードバックを反映して改稿。** CRITICAL・MEDIUM 全件を修正。Kimi の初稿から大幅に書き直し、著者の内省（「397問に自分も引きずられていた」等）を追加し、手法の限界にも言及した
5. **改稿版を Kimi に再度フィードバック。** 今回はペルソナ指定なしで群知能の自律判断に任せた。本文で述べたバズ分析レビューでは4観点を指定したが、2回目はあえて制約を外した。Kimi は A 評価（公開推奨）を返し、冒頭の数値整合性など軽微な修正3点を指摘した
6. **Kimi のフィードバックを反映して最終版を完成**

Kimi の使い分けで見えたこと: ピアレビュー（批評・分析）では Kimi の群知能が多角的な視点を返し、350行もの詳細なフィードバックを生成した。一方、記事執筆（文章の生成）では Claude の方が圧倒的に品質が高かった。同じモデルでも、批評と生成では能力の発揮のされ方が異なる。
